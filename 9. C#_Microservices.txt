Question : Explain about Microservices architecture :
1. Microservices architecture is a design approach where an application is broken down into a collection of loosely coupled, independently deployable services, 
2. Each microservice is typically implemented as a separate project and each service is responsible for a specific business functionality. 
3. They can be developed independently, deployed independently and they can be scaled independently, In case of monolithic services we cannot do all these things, all functionality is usually within a single project.
4. If suppose if you want to create an application with micro service architecture, we can develop using different technologies/programming languages, we can use .net, we can use use java , we can use one technology for one micro service and another technology for another microservice. 
5. When it comes to scaling in monolithic services, the complete application has to be scaled, but incase of micro service, we can just scale the specific microservice that is required{under load, optimizing resource usage and performance.}.
6. Reduced Downtime

Why Microservices?
Microservices make it easier to develop, test, and deploy isolated parts of your application. Once deployed, each microservice can be independently scaled as needed.

Design Principles of Microservices
1. Agility: The core idea of microservices is to make the application swiftly respond to evolving requirements & enable rapid development, deployment, and adaptation to changes in a dynamic and flexible manner.
2. Decomposition: The application is decomposed into smaller, manageable services; each focused on a specific business function.
3. Independence: Microservices are designed to be independently deployable.
4. Autonomy : Each service team has a high degree of autonomy. They can choose the technology stack, database, and deployment strategies
5. API-based Communication: Services communicate with each other through well-defined RESTful APIs, often over lightweight protocols like HTTP or gRPC.
6. Scalability:Microservices enable horizontal scalability.
7. Resilience: Resilience is about making the service to be able to handle failures gracefully, and the failure of one service should not bring down the entire application. Techniques like circuit breakers and retries are often employed.
8. Testing: Each service can be tested individually, without affecting other services.

Question : Characteristics of Each Microservice Project:
1. Independent Deployment:
    * Each service is developed and deployed independently. For example, you could deploy a new version of OrderService without redeploying UserService or PaymentService.
2. Own Database (Optional):
    * In many cases, each microservice will have its own database. This prevents tightly coupling data models and allows each microservice to scale and manage its data independently.
    * For example, UserService could use SQL Server for storing user details, while OrderService might use MongoDB to store orders.
3. Communication:
    * Microservices communicate with each other over a network, using HTTP APIs, gRPC, or message queues. So OrderService might call UserService via HTTP to fetch user details (like an HTTP request to GET /api/users/{id}).
4. Loose Coupling:
* Microservices are loosely coupled because each service manages its own logic, data, and state. One service can be updated without necessarily impacting others.
* For example, you could update the logic in PaymentService without requiring a change to OrderService or UserService, as long as the API contract remains the same.
5. Scalability:
* Microservices can be scaled independently based on demand. If UserService is under heavy load while PaymentService is not, you can scale UserService without scaling other services



Regular (Monolithic) Architecture:
Monolithic architecture is an architectural approach in software development, in which the application is built as a single, tightly integrated unit, where all components and functions of the application such as UI, business logic and data access are tightly-coupled and run within a single process or application instance with a single data store.

* Single project: Everything (user management, orders, payments, etc.) resides in the same project or application.
* Tightly coupled: All parts of the application are interconnected. If you need to make a change to the order logic, you might need to touch multiple areas of the codebase (e.g., UI, API, services).
* Deployment: The entire application is deployed as one unit. If you update one part of the system, you typically have to redeploy the whole thing.
Microservices Architecture:
Each microservice is implemented as a separate project in its own repository. These services are independently deployable, meaning you can deploy or update one service without affecting others.

Question : What is an API Gateway?
An API Gateway serves as a single entry point for all client requests. It acts as a reverse proxy that routes requests to the appropriate microservices and can also handle other cross-cutting concerns like:
* Authentication & Authorization
* Load Balancing
* Caching
* Rate Limiting
* Request Aggregation

* Rate Limiting : This refers to controlling the number of requests a client can make to the API within a certain time window. It's implemented to prevent overloading of services and to ensure fair use of the API by different clients.
=> Example: A client can only send 100 requests per minute, and if it exceeds that limit, the API Gateway will reject further requests until the rate limit resets.
 // Add Rate Limiting
builder.Services.AddRateLimiter(options =>
{
    options.AddPolicy("FixedWindowPolicy", policy =>
    {
        policy.FixedWindow(100, TimeSpan.FromMinutes(1));  // 100 requests per minute
    }); 

* Request Aggregation: The API Gateway can aggregate responses from multiple microservices and combine them into a single response. This is useful when a client needs data from multiple services
=> Example: A client may need data from both the Order and Payment microservices. The API Gateway can make both requests and combine the results into a single response, reducing the complexity for the client.

Question : How do micro services communicate with each other?â€¨Communication between microservices can be achieved using a variety of patterns and protocols, depending on the specific use case. Generally, microservices communicate via HTTP-based APIs, messaging systems, or event-driven architectures. Below are the main ways these microservices interact:

1. RESTful HTTP APIs:
* HTTP APIs (REST): Microservices expose RESTful APIs, allowing other services to call them via HTTP requests[using http client]. This is the most common method, suitable for synchronous communication. [Synchronous communication]
* Example: A payment microservice might expose a POST API to initiate a payment, and an order service can call this API to process payments. This is a typical synchronous interaction.

2. Asynchronous Communication (Message Brokers):
* Message Brokers : Microservices can communicate asynchronously by using message brokers like RabbitMQ, Kafka, or Azure Service Bus.. In this case, services publish messages to a queue or topic, and other services subscribe to listen for those messages. This decouples services and helps improve scalability and fault tolerance.
* Example: An order service might publish a message to a queue after an order is created. The inventory service then listens for these messages and updates the stock accordingly.

3. gRPC (Remote Procedure Call):
* Synchronous communication with high performance: gRPC is a framework developed by Google for efficient and high-performance communication. It's often used when low-latency, high-throughput communication is required between microservices.
* gRPC uses Protocol Buffers (protobuf) for serializing data, which is faster and more compact than JSON.

4. Event-Driven Architecture:
* Event Bus: Microservices can communicate using events, especially in an event-driven architecture. When something happens in one service, it emits an event, and other services can listen to this event and react accordingly.
* Example: When a customer places an order, the order service emits an OrderPlaced event. The inventory service listens for this event and updates the stock levels accordingly. This is a form of asynchronous communication.

5. API Gateway:
* An API Gateway serves as a single entry point for external clients or even internal microservices. It can handle routing, load balancing, authentication, and rate limiting.
* When microservices need to communicate with each other, the API Gateway might aggregate responses or route requests to the appropriate microservice.
* In .NET, an API Gateway can be implemented using Ocelot or Azure API Management.

Question : How do you make sure micro services can handle more users as becomes popular?
Load balancing
Horizontal scaling
Auto scaling
Caching
Database scaling
Asynchronous processing

* Question: What is Azure App Service, and how does it differ from other Azure compute services like Azure Functions and Virtual Machines?
=> Azure App Service is a fully managed platform for hosting web applications, RESTful APIs, and mobile backends. It provides scalability, high availability, built-in security, and integration with Azure DevOps and GitHub.
Azure App Service vs. Virtual Machines:
VMs require manual OS and infrastructure management, whereas App Services handle infrastructure automatically.
Azure App Service vs. Azure Functions:
App Services run long-running applications with stateful processing.

* Question: Can you explain the different types of hosting plans available in Azure App Service?
Azure App Service provides different hosting plans:
Free & Shared â€“ Limited resources, good for testing.
Basic â€“ Entry-level, good for small apps with low traffic.
Standard â€“ Autoscaling, custom domains, SSL.
Premium â€“ Advanced scaling, better performance, networking integration.
Isolated (App Service Environment - ASE) â€“ Dedicated environment with private networking

* Question: What are the deployment options available for an Azure App Service? Which do you typically prefer?
Azure App Service supports multiple deployment methods:
Azure DevOps Pipelines â€“ Preferred for enterprise CI/CD.
GitHub Actions â€“ Ideal for GitHub-based workflows.
ZIP Deploy / FTP â€“ Quick, manual deployments.
ARM Templates / Terraform â€“ Infrastructure as Code (IaC) deployments.
Azure CLI / PowerShell â€“ Scripted deployments.
Preferred Approach: Azure DevOps or GitHub Actions for automated CI/CD.

SECTION 3. Scaling and Performance

* Question: How do you scale an Azure App Service to handle more traffic or load?
Scaling refers to the process of adjusting the number of resources allocated to a service. In Azure App Service, you can scale your application both vertically and horizontally.

Vertical Scaling (Scale-Up): Increase the instance size (CPU, memory). or Increasing the resources (CPU, RAM) of your existing instances. You may scale from a lower tier (e.g., Basic) to a higher tier (e.g., Premium).Used for quick fixes.

Horizontal Scaling(Scale-Out): Scale out by increasing the number of instances running your app to handle varying traffic loads. You are adding  instances of your app to handle increased or decreased load.(preferred for distributed workloads)

Considerations:
Traffic patterns (predictable vs. sudden spikes).
Cost vs. Performance trade-offs.
Autoscaling vs. manual scaling.

How to Scale in and Scale out in Azure App Service?

How to Scale in and Scale out in Azure App Service?
In Azure App Service, you can scale your app either manually or automatically:
=> Manual Scaling: You manually specify the number of instances for your app.
 Go to the Azure Portal > App Service > Scale out (App Service Plan) and manually set the number of instances.
=> Automatic Scaling (Autoscale): Azure can automatically scale your app based on traffic, CPU usage, memory usage, etc. You set a scaling rule, and Azure adjusts the number of instances as needed.
Example Auto-scaling rule: If CPU usage > 70% for 5 minutes, scale out by 2 instances.
 Azure Autoscale Configuration Example:
Scenario: Auto-scale an app when CPU utilization exceeds 70% for 10 minutes.
Go to App Service > Scaling > Autoscale.
Add a rule to scale the app out when CPU > 70%.
Set a minimum of 2 instances and a maximum of 10 instances.

* Question: What are Auto-scaling rules in Azure App Service, and how would you configure them?
Auto-scaling rules adjust resources dynamically based on load.
Metrics-based Scaling:
Scale out/in based on CPU %, Memory %, or Request Count.
Schedule-based Scaling:
Define scaling policies for peak and off-peak hours.
Configuration:
Go to Azure Portal â†’ App Service â†’ Scale Out.
Define rules (e.g., Scale out when CPU > 70% for 5 min).

What Are Deployment Slots?
Deployment slots in Azure App Service allow you to deploy different versions of your app (e.g., development, staging, production) into different environments without affecting the production environment. This is particularly useful in continuous delivery and blue-green deployments.
Benefits of Deployment Slots:
Zero-Downtime Deployments: You can deploy updates to a staging slot and, once verified, swap it to production without downtime.
Testing in Production-Like Environment: The staging slot runs in the same environment as production, so you can verify how the application behaves under real-world conditions.
Rollback: If something goes wrong after a slot swap, you can simply swap back to the previous version.

* Question: How do you set up CI/CD for an Azure App Service?
Use Azure DevOps Pipelines or GitHub Actions.
Set up Build & Release pipeline.
Enable Auto-deployment triggers.
Rollback Strategy â€“ Deploy previous successful build on failure.

Question : When to choose microservices architecture over monolithic architecture?
Scenarios where microservice architecture is preferred over monolithic architecture:â€¨Scalability: If your application needs to scale certain parts independently (e.g., high traffic to one service), microservices allow you to scale individual services, rather than the entire application.
Flexibility: Microservices give you the freedom to use different technologies or frameworks for different services, based on specific needs.
Development Velocity: When working with large teams, microservices allow parallel development across multiple services, speeding up overall delivery.
Team Structure: If your development teams are large and work on different business domains, microservices align well as each team can own and manage its own service.
Complexity: When the application is large and complex, breaking it down into smaller, manageable microservices can make it easier to develop, deploy, and maintain.
Maintenance: Microservices offer easier maintenance since individual services can be updated, deployed, and monitored independently, reducing risk and downtime.


Question : When to choose monolithic architecture over microservice architecture?
Scenarios where monolithic architecture is preferred over microservice architecture:
Limited Complexity: For small to medium-sized applications with low complexity, a monolithic approach is simpler and faster to develop, deploy, and maintain.
Development Speed: If you need to get a product to market quickly, a monolithic architecture often allows for faster initial development due to its simplicity and fewer integration points.
Resource Constraints: Microservices require more infrastructure and management overhead (e.g., containers, orchestration tools), so if resources are limited, a monolithic approach may be more cost-effective.
Performance: Monolithic applications can sometimes perform better, as they avoid the overhead of inter-service communication and network latency that comes with microservices.
Team Size: If the team is small, a monolithic architecture can be easier to manage and develop, as thereâ€™s less coordination needed between teams.
Learning Curve: For teams unfamiliar with distributed systems or microservices, a monolithic approach has a lower learning curve and is easier to implement and manage.

Question: What Are Client Certificates?
Client certificates are mainly used to authenticate the client identity to the server. There is no encryption of data in the case of Client certificates. Server Certificates are usually based on PKI. The Client certificates are also based on PKI.
A type of digital certificate that is used by client systems to make authenticated requests to a remote server is known as the client certificate. Client certificates play a very important role in many mutual authentication designs, providing strong assurances of a requesterâ€™s identity.

Q1) . A sudden and unexpected load spike is causing some of your microservices to fail under pressure. How would you address this problem while ensuring minimal disruption?
Ans) You can follow below approaches:
ðŸ’ŽAutoscaling: Implement horizontal autoscaling to dynamically scale up your microservice instances in response to increased traffic. For cloud environments like AWS or Kubernetes, set up automatic scaling policies based on CPU, memory usage, or custom metrics like request count. This ensures additional resources are allocated automatically when load spikes occur.
ðŸ’ŽRate Limiting and Throttling: Introduce rate limiting to cap the number of requests per user or client. This prevents any single user from overwhelming the service. You can also use request throttling to slow down the rate of incoming requests when the system detects heavy traffic, ensuring critical services remain available.
ðŸ’ŽCaching: Leverage caching (e.g., Redis, Memcached) to reduce the load on the backend. Frequently accessed data can be served from cache rather than hitting the database or core service repeatedly.
ðŸ’ŽCircuit Breaker Pattern: Implement a circuit breaker to prevent cascading failures. When a microservice is under extreme load, the circuit breaker opens, failing fast, and sending an error response without consuming more resources.
ðŸ’ŽLoad Balancing: Ensure that requests are distributed evenly across all available instances using a load balancer (e.g., NGINX, AWS ELB). Proper load balancing helps distribute traffic more effectively and ensures that no single instance gets overwhelmed.

2 Q) You need to deploy a new version of a microservice without disrupting the existing users. How would you manage service versioning and ensure a smooth transition?
Ans)
ðŸ’ŽVersioning Strategy: Use Semantic Versioning (e.g., v1, v2, etc.) to indicate breaking and non-breaking changes. It allows multiple versions of your API to run simultaneously, giving users time to transition.
ðŸ’ŽBlue-Green Deployment: In this strategy, you deploy the new version (blue environment) alongside the existing version (green environment). Initially, all traffic goes to the green environment. Once the new version is tested and confirmed stable, the traffic is switched over to the blue environment. If issues arise, you can switch.
ðŸ’ŽCanary Releases: Gradually release the new version to a small subset of users or instances (canaries) and monitor the performance. If successful, the rollout is expanded to the rest of the users. This ensures any critical issues are identified early with minimal impact.


What is a Container?
A container is a lightweight, portable, and self-contained environment used to run an application. It includes everything the app needsâ€”such as code, runtime, libraries, and system toolsâ€”so it can run consistently across any environment. For .NET 8 ASP.NET Web API apps, containers allow you to package the entire app with its dependencies, ensuring it runs the same way in development, testing, and production, whether on your local machine or in the cloud (like Azure).

Why Do We Use Docker in Microservices?
In a microservices architecture, each service is a small, independent application that can be developed, deployed, and scaled separately. Docker is used to package these microservices into isolated, lightweight containers. With Docker, we can encapsulate each microserviceâ€”like an ASP.NET Core Web APIâ€”along with all its dependencies into a container, making the service independent of the underlying environment. This ensures that it will run seamlessly across different platforms, whether itâ€™s in a developer's local environment, testing, or deployed in Azure.

What is Docker?
Docker is a platform that allows you to package your applications and their dependencies into containers. A Docker container is a lightweight, resource-isolated environment that includes everything your application needs to runâ€”like the .NET runtime, libraries, and application code. This ensures that your .NET 8 ASP.NET Web API application will work the same way in development, test, and production environments, even if the underlying infrastructure or cloud platform (such as Azure) changes. It eliminates the "works on my machine" problem and simplifies deployment and scaling. or
Docker is a containerization tool that packages applications with their dependencies for consistent and efficient deployment across diverse environments.

Introduction to RabbitMQ
RabbitMQ is an open-source message broker that facilitates the exchange of messages between microservices to implement indirect / asynchronous communication.

Terminology
Producer/Publisher Microservice:  Publishes messages to an exchange.
Exchange:   Routes messages to the appropriate queues based on bindings.
Bindings: Connections between routing keys (events or actions) and message queues within a specific exchange.
Message Queues:    Hold the messages until they are consumed.
Consumer Microservices:   Consume (read) messages from the queues.


RabbitMQ Exchanges
RabbitMQ Exchanges
Fanout Exchange: Routes messages to all queues bound to the exchange.
Direct Exchange: Routes messages to queues based on an exact match between the message's routing key and the queue's binding key.
Headers Exchange: Routes messages based on matching message headers (metadata), instead of the routing key.
Topic Exchange: Routes messages to queues based on pattern matching between the routing key and the binding key using wildcards (* for a single word and # for zero or more words).
Fanout Exchange: Routes messages to all queues bound to the exchange.
Direct Exchange: Routes messages to queues based on an exact match between the message's routing key and the queue's binding key.
Headers Exchange: Routes messages based on matching message headers (metadata), instead of the routing key.
Topic Exchange :Routes messages to queues based on pattern matching between the routing key and the binding key using wildcards (* for a single word and # for zero or more words).
