Using Kafka in an ASP.NET Core Web API

At its core, Kafka is a distributed streaming platform designed to handle large volumes of data in real-time. Its exceptional performance, fault tolerance, and scalability make it a pivotal component in event-driven architectures.

Understanding Kafka and Event-Driven Architecture
Kafka utilizes a publish-subscribe model. In this model, producers publish messages to topics, and consumers subscribe to those topics to receive the messages. This model allows for seamless communication between different system components and promotes the creation of scalable and responsive applications.

Apache Kafka is a distributed event streaming platform used for building real-time data pipelines and streaming applications. It's commonly used in enterprise-level projects for handling high throughput data feeds, message queues, and event-driven architectures. In a .NET Core environment, Kafka serves as a backbone for microservices communication, event sourcing, and data streaming.

Basic Kafka Components:
1. Producer: An application or service that sends messages to Kafka topics.
2. Consumer: An application or service that reads messages from Kafka topics.
3. Broker: Kafka server that manages the messages. Multiple brokers can form a Kafka cluster.
4. Topic: Logical channels where messages are sent and read. Each message belongs to a topic.
5. Partition: A topic is split into partitions. This allows Kafka to scale by distributing the topic's data across multiple brokers.
6. Consumer Group: A group of consumers that work together to read messages from one or more partitions of a topic. Each partition is read by only one consumer in the group at a time.

￼


Kafka Architecture in Detail:
1. Kafka Producer:
    * Producers push records (messages) to Kafka topics.
    * Kafka producer clients are responsible for connecting to the Kafka cluster, serializing messages, and pushing them to the appropriate topic partitions.
    * A Kafka producer can send messages to multiple partitions within a topic to distribute load and ensure parallelism.
2. Kafka Broker:
    * The Kafka cluster consists of multiple brokers that handle incoming messages, store them on disk, and serve the data to consumers.
    * Each broker in the cluster handles partitions of topics.
    * Brokers maintain messages for a configurable retention period and ensure fault tolerance through replication.
3. Kafka Consumer:
    * Consumers read records from Kafka topics.
    * A consumer can belong to a consumer group, meaning that Kafka will ensure that each partition's data is processed by only one consumer in the group, providing load balancing and parallel processing.
4. Kafka Zookeeper:
    * Kafka uses Zookeeper to manage its metadata, broker coordination, and cluster management.
    * Zookeeper helps to maintain information about Kafka brokers, partitions, and consumer group offsets.
5. Kafka Topic:
    * Topics are logical entities that Kafka producers write to and consumers read from. Kafka topics can have multiple partitions.
    * Kafka ensures message order within a partition but does not guarantee order across partitions.
6. Kafka Partition:
    * Each topic is split into partitions for better scalability and load balancing.
    * Partitions allow Kafka to horizontally scale by distributing the topic's data across multiple brokers.
7. Replication:
    * Kafka provides fault tolerance by replicating partitions across multiple brokers.
    * For example, if a partition's leader broker fails, one of the replica brokers can take over as the leader.



Kafka comprises several components that work together to provide a reliable and scalable messaging system:

In this diagram, we have producers that create new events or data messages and publish them to specific topics in Kafka.
The central component is the Kafka Cluster, comprising multiple Brokers for high availability, fault tolerance, and load balancing. The cluster receives these messages and stores them. The Kafka Cluster of this streaming data platform works as a center where producers send data, while consumers receive such information. Various partitions divide each topic, and Kafka distributes them across multiple brokers in a cluster to ensure both replication and parallelism.

Zookeeper plays the vital role of managing and coordinating the Kafka Brokers. It helps maintain a list of brokers, tracks the status of nodes, and, most importantly, operates leader elections for partition.
The final element represented in the diagram is the consumers. Consumers subscribe to one or more topics in the Kafka Cluster, consume the data feed, and process it. We can organize consumers into consumer groups to optimize this process, enabling load balancing across the system. The consumers act as the end receivers of the streaming channel.

Event-driven Architecture
Event-driven is an architectural pattern that organizes a system as loosely coupled components communicating and coordinating through events.

People often link Kafka with microservices because it helps services talk to each other without being too dependent.
However, Kafka isn’t only for microservices. It fits into all kinds of setups, even with big applications starting to adopt the event-driven approach.


Kafka’s importance in event-driven applications arises from its high throughput, fault tolerance, and event sequencing capabilities. Because events are foundational to such architectures, Kafka’s efficient event management is crucial for building reliable, scalable systems.

Kafka Interview Questions with Simple Answers
1. What is Apache Kafka?
    * Apache Kafka is an open-source distributed event streaming platform used to build real-time data pipelines and streaming applications. It’s designed to handle large amounts of data in a fault-tolerant, scalable, and high-throughput way.
2. **What are the main components
of Apache Kafka?**
* Producer: Sends messages to Kafka topics.
=> EX: 
STEP 1: var config = new ProducerConfig { BootstrapServers = _bootstrapServers }; //// Set up Config OBJECT
STEP 2: var producer = new ProducerBuilder<Null, string>(config).Build() // Create a single producer instance
STEP 3: var _message =  new Message<Null, string> { Value = message }  // Initialize mesage
STEP 4: var result = await producer.ProduceAsync(_topic, _message); //Publish message with topic
* Consumer: Reads messages from Kafka topics.
=> EX: 
KafkaConsumerService:
STEP 1: var consumeConfig = new ConsumerConfig {}
STEP 2: var consumer = new ConsumerBuilder<Ignore, string>(consumeConfig).Build();
STEP 3: consumer.Subscribe(_topic);
STEP 4:var consumeResult = consumer.Consume();

* Broker: Manages the message storage and serves data to consumers.
* Topic: A channel to which messages are sent by producers and consumed by consumers.
* Partition: A topic is split into partitions for scalability and parallelism.
* Consumer Group: A group of consumers that share the load of reading from a topic.
* Zookeeper: Manages metadata, broker coordination, and cluster state (though Kafka is moving towards KRaft mode without Zookeeper).
3. What is the difference between a topic and a partition in Kafka?
    * A topic is a logical entity to which producers send messages and consumers read from. A partition is a physical split of a topic that allows Kafka to scale by distributing data across multiple brokers. Each partition can be replicated for fault tolerance.
4. What is replication in Kafka?
    * Replication refers to creating copies of a partition across multiple brokers. This ensures data durability and availability. If a broker or partition fails, another replica can take over to prevent data loss.
5. How does Kafka ensure message durability?
    * Kafka stores messages on disk, and data can be replicated across multiple brokers. The retention period and replication factor ensure that messages remain available and durable.
6. What is a consumer group in Kafka?
    * A consumer group is a collection of consumers that share the task of reading from a topic. Each partition within the topic is consumed by only one consumer in the group at a time, enabling parallel processing and load balancing.
7. What are Kafka’s key advantages?
    * Scalability: Kafka can scale horizontally by adding brokers and partitions.
    * Fault tolerance: Through replication, Kafka ensures that data is not lost if brokers fail.
    * High throughput: Kafka is optimized for high-volume, low-latency messaging.
    * Durability: Kafka persists messages to disk and provides configurable retention.
8. How does Kafka handle message ordering?
    * Kafka guarantees message ordering within a partition but does not guarantee global ordering across partitions. If order is important, ensure that all related messages are sent to the same partition.
9. What is the role of Zookeeper in Kafka?
    * Zookeeper is used for managing Kafka’s metadata, leader election for partitions, and broker coordination. It helps Kafka maintain state consistency across the cluster. However, newer versions of Kafka are moving to KRaft mode, eliminating the need for Zookeeper.
10. What is the role of Kafka Streams?
    * Kafka Streams is a library for stream processing that allows you to build real-time applications on top of Kafka. It provides APIs for processing streams of data, such as filtering, joining, and aggregating data, and outputs the results back into Kafka.
11. What is the difference between Kafka and traditional messaging queues like RabbitMQ or ActiveMQ?
    * Kafka is designed for high throughput and durability with horizontal scalability, while traditional message queues focus on low-latency message delivery. Kafka is optimized for handling large volumes of data and supports both stream processing and batch processing.
12. What is "at least once" and "exactly once" delivery in Kafka?
    * At least once: Kafka guarantees that messages are delivered at least once but may be delivered more than once in some failure scenarios.
    * Exactly once: Kafka ensures that a message is delivered exactly once, even in the event of failures. This requires special configuration and idempotent producers.
13. How can you scale Kafka for high throughput?
    * Kafka can be scaled by increasing the number of partitions and brokers. More partitions allow greater parallelism, and more brokers distribute the load across the cluster. Consumer groups also help scale the consumption of data.
14. What is message retention in Kafka?
    * Kafka retains messages for a configurable period (or until a disk space limit is reached). After the retention period expires, the messages are deleted, freeing up space for new messages. The retention policy can be based on time or size.
15. How does Kafka handle backpressure?
    * Kafka uses a consumer lag mechanism where consumers read data at their own pace. If a consumer falls behind, Kafka will retain messages in the partition until the consumer catches up. However, producers may experience backpressure if the Kafka brokers are overwhelmed, and they will wait until there is space available.

1. What is Apache Kafka, and how does it work?
For beginners:
Kafka is an open-source messaging system that helps different services communicate with each other. It stores and transmits large amounts of data in real-time. Imagine Kafka as a giant post office: messages (data) are sent by producers, stored in topics, and then read by consumers.

For experienced developers:
Kafka is a distributed, high-throughput event streaming platform. It uses a publish-subscribe model where producers push messages to Kafka topics. Kafka brokers store messages in partitions for scalability. Consumers read messages from topics in real-time, and Kafka can retain data for a configurable period to allow consumers to catch up or reprocess messages. Kafka guarantees fault tolerance and message durability through replication and partitioning.

2. What are the core concepts in Kafka?
For beginners:

Producer: Sends messages to Kafka topics.
Consumer: Reads messages from Kafka topics.
Topic: A category or stream of data where messages are published.
Partition: A topic is divided into partitions to spread the load.
Broker: A Kafka server that stores and manages messages.
Zookeeper (optional in newer versions): Manages Kafka brokers and ensures the system works smoothly.
For experienced developers:

Producer: Responsible for pushing messages to Kafka. It can send messages to specific partitions or allow Kafka to assign them.
Consumer: Pulls messages from Kafka, often in consumer groups to share the load.
Topic: Kafka topics are logical streams of data, organized into partitions for parallel processing and scalability.
Partition: Kafka's partitioning mechanism allows horizontal scaling by spreading data across brokers.
Broker: Kafka brokers handle data storage, retrieval, and replication. They form a Kafka cluster.
Zookeeper: Handles Kafka broker metadata and leader election for partitions. In newer versions (KRaft mode), Zookeeper is no longer required.
3. What is the difference between Kafka and traditional messaging systems like RabbitMQ?
For beginners:
Kafka is designed for high throughput, durability, and scalability, whereas systems like RabbitMQ are more focused on traditional message queuing with a simpler setup. Kafka excels in handling large volumes of data and real-time streaming, whereas RabbitMQ is better suited for request-response patterns and smaller, more transactional workloads.

For experienced developers:
Kafka is an event streaming platform that can handle high-throughput, distributed, and fault-tolerant data streams. It uses log-based storage and allows both real-time processing and batch processing. In contrast, RabbitMQ is a message broker primarily focused on AMQP-based queuing with support for message acknowledgments, routing, and retry policies. Kafka's partitioned model scales horizontally, whereas RabbitMQ typically scales vertically.

4. What is Kafka's message retention policy? How does it work?
For beginners:
Kafka stores messages for a specified amount of time (configured by retention period) or until the disk space is full. After that, old messages are deleted to free up space.

For experienced developers:
Kafka retains messages based on time (e.g., 7 days) or size (e.g., 1GB per partition). Unlike traditional message queues, Kafka doesn’t delete messages once they are consumed; instead, it retains them in logs for a specified duration or until the log reaches its size limit. This design allows consumers to re-read messages if needed (e.g., for recovering from a failure).

5. How does Kafka ensure fault tolerance and high availability?
For beginners:
Kafka makes sure messages are not lost by storing copies of messages on multiple servers (called replication). Even if one server fails, Kafka can still serve the messages from the other servers.

For experienced developers:
Kafka achieves fault tolerance by replicating each partition across multiple brokers. The leader of each partition is responsible for handling reads and writes, while followers replicate the data. If a broker or leader fails, Kafka can elect a new leader from the replicas. This ensures high availability and durability of the data.

6. What is Kafka's exactly-once delivery semantics, and how does it work?
For beginners:
“Exactly-once” means that each message is processed exactly one time. Kafka ensures this even when failures happen. Without exactly-once semantics, messages could be processed multiple times, which can cause duplication.

For experienced developers:
Kafka’s exactly-once semantics (EOS) ensures that messages are neither lost nor duplicated. This is achieved through a combination of idempotent producers and transactional consumers. The producer assigns a unique transaction ID to a batch of messages, and Kafka ensures the batch is either fully written or not written at all. Consumers, in turn, commit offsets only once processing is successfully completed, ensuring no reprocessing of the same message.

7. How does Kafka handle message ordering?
For beginners:
Kafka guarantees the order of messages within a single partition. If you have multiple partitions, the order across partitions is not guaranteed.

For experienced developers:
Kafka guarantees message ordering per partition. Each partition is an ordered log, and messages within a partition are consumed in the order they were produced. If strict ordering is required across multiple messages, producers must ensure that related messages are sent to the same partition (using a partition key). Kafka does not guarantee ordering across partitions, so it’s important to model the system accordingly.

8. Explain the concept of consumer groups in Kafka.
For beginners:
A consumer group is a group of consumers that work together to read from Kafka topics. Each consumer in the group reads from a different partition, so multiple consumers can process messages in parallel.

For experienced developers:
A consumer group allows for horizontal scaling by distributing the consumption of messages across multiple consumers. Each consumer in the group is assigned one or more partitions, and each message in a partition is processed by only one consumer within the group. This provides scalability while ensuring that each message is processed only once. Kafka ensures that if a consumer fails, another consumer from the group will take over.

9. How does Kafka scale horizontally?
For beginners:
Kafka can handle more data by adding more servers (brokers) to the cluster. It divides topics into smaller chunks called partitions, and each partition can be handled by a different server. This way, Kafka can handle more traffic by adding more machines.

For experienced developers:
Kafka scales horizontally through partitioning. Each topic is split into multiple partitions, and each partition is distributed across brokers. By adding more brokers, Kafka can increase the number of partitions, which enables the system to handle higher throughput and greater volumes of data. Kafka also uses replication to ensure fault tolerance as the system scales.

10. How can Kafka be used for communication between microservices?
For beginners:
Kafka allows microservices to talk to each other by sending messages to topics. When one service sends a message, others can listen to the topic and process it asynchronously. This is great for building event-driven systems.

For experienced developers:
Kafka supports event-driven architectures by enabling services to communicate asynchronously via topics. Producers publish events to Kafka, and consumers process those events. Kafka decouples services, ensuring that they don't need to be directly aware of each other. By using consumer groups, Kafka allows multiple services to share the workload, ensuring scalability and resilience in microservices architectures.

These answers should give a broad understanding of how Kafka works, both for someone new to the topic and someone with experience. Let me know if you'd like more in-depth answers on any particular question or topic!


You said:
I need answers for all 30 questions in similar way
ChatGPT said:
ChatGPT
Absolutely! I'll continue the same pattern for the remaining questions, providing simple explanations for beginners and more technical answers for experienced developers.

11. How does Kafka support eventual consistency in microservices?
For beginners:
Kafka helps microservices stay consistent even if they are not connected all the time. Services send messages to Kafka, and other services can read these messages later to keep their data in sync.

For experienced developers:
Kafka enables eventual consistency by decoupling microservices through event-driven architecture. Each service publishes events (domain events) to Kafka topics, and other services consume these events to update their state. Since Kafka retains messages for a period, it ensures that services can eventually process events and synchronize their data, even in the event of temporary service failures or delays.

12. What are some of the patterns you can implement with Kafka in a microservices system?
For beginners:
Some common patterns are:

Event Sourcing: Services record every change as an event.
CQRS (Command Query Responsibility Segregation): Use Kafka for separating reading and writing data.
Saga Pattern: Break down complex transactions into smaller parts, each with its own event.
For experienced developers:

Event Sourcing: Every state change is captured as an event, which is stored in Kafka, enabling full audit trails and rebuilding state by replaying events.
CQRS: Commands (writes) and queries (reads) are handled by separate services. Kafka enables asynchronous updates to different microservices and data models.
Saga Pattern: Long-running business transactions across multiple services can be coordinated by publishing events to Kafka, where each service handles part of the transaction and publishes the outcome.
13. What are the advantages of using Kafka in a microservices ecosystem?
For beginners:
Kafka helps microservices communicate easily, scale, and stay reliable. It makes it easier to handle large amounts of data, enables real-time communication, and decouples services, meaning one service doesn't need to know about another.

For experienced developers:
Kafka provides:

High throughput: Kafka can handle large volumes of data with low latency, making it ideal for real-time microservices communication.
Fault tolerance and durability: Kafka’s replication and partitioning mechanisms ensure messages are not lost and can be recovered.
Decoupling of services: Producers and consumers are loosely coupled, enabling microservices to evolve independently.
Scalability: Kafka’s distributed architecture allows it to scale horizontally to meet the growing demands of the microservices ecosystem.
14. What are some challenges when using Kafka in a microservices-based system?
For beginners:
Some challenges include:

Ensuring services handle message failures properly.
Deciding how long to keep messages.
Handling different versions of messages.
For experienced developers:

Message schema evolution: Handling changes in the format of messages over time (e.g., versioning).
Exactly-once semantics: Ensuring messages are processed only once, even during failures, can be challenging with high-volume systems.
Consumer state management: Keeping track of consumer offsets and ensuring that no messages are missed or processed multiple times.
Data consistency across services: Ensuring that microservices remain eventually consistent when processing Kafka events.
15. How do you ensure data consistency and handle failures when using Kafka in microservices?
For beginners:
Kafka ensures data consistency by letting each service process messages at its own pace. If a service crashes, it can pick up from where it left off by reading messages from Kafka again. This helps keep data consistent.

For experienced developers:

Idempotency: Consumers should be idempotent, meaning they can safely process the same message multiple times without side effects.
Transactional writes: Kafka supports transactions, ensuring that a set of messages are either fully written or not at all.
Retries and dead-letter queues (DLQ): Use retry mechanisms and DLQs for failed messages to ensure eventual consistency.
Consumer offset management: Ensuring consumer offsets are committed only after successful message processing prevents data loss or duplication.
16. Can you explain how to implement a "saga" pattern with Kafka for managing distributed transactions in microservices?
For beginners:
A saga is a way to manage big tasks by splitting them into smaller steps. Each step in a saga sends an event to Kafka, and each service listens for events to perform its part. If something goes wrong, earlier steps can be undone.

For experienced developers:
The Saga pattern in Kafka is implemented using a series of events that represent each step of a distributed transaction. Each service that participates in the saga listens to events and publishes new events upon completing its step. If one step fails, compensating transactions are triggered to undo earlier steps. Kafka ensures asynchronous message delivery, enabling each service to react to events, while maintaining fault tolerance and distributed consistency.

17. How would you integrate Kafka with a .NET microservices-based application?
For beginners:
You can use a Kafka library like Confluent.Kafka in .NET. Producers can send messages to Kafka, and consumers can listen to those messages in a service. Both producers and consumers are configured to connect to a Kafka cluster.

For experienced developers:

Use Confluent.Kafka (official .NET client) to integrate Kafka with .NET microservices.
Configure Kafka producers to serialize data into formats like JSON, Avro, or Protobuf, and send messages to topics.
Set up consumers to subscribe to topics and process messages asynchronously.
Implement proper error handling, offset management, and consumer group configuration for scaling.
18. What .NET libraries or frameworks can be used to work with Kafka?
For beginners:
The most common library is Confluent.Kafka, which is the official .NET client for Kafka. It allows you to create Kafka producers and consumers easily in your .NET applications.

For experienced developers:

Confluent.Kafka: The primary .NET client, widely used for Kafka integration in .NET applications.
KafkaNet: An alternative client for Kafka, but less commonly used than Confluent’s official library.
Akka.NET: While not Kafka-specific, Akka.NET can be used with Kafka to build distributed systems and actor-based concurrency models.
MassTransit: A popular framework for building distributed applications in .NET that can integrate with Kafka as a message transport.
19. How do you manage and handle Kafka consumer offsets in a .NET application?
For beginners:
Offsets are like bookmarks in Kafka. They tell a consumer where it left off reading. You can let Kafka manage offsets automatically, or you can manually commit offsets to control when messages are marked as processed.

For experienced developers:

Kafka supports automatic offset committing (every auto.commit.interval.ms) and manual offset committing using the CommitAsync() method for precise control over when a message is marked as consumed.
Consumer group offsets are tracked in a special internal Kafka topic (__consumer_offsets).
You can control offset storage (Kafka vs. external systems like databases or distributed caches).
For exactly-once semantics, ensure that offsets are committed only after processing the message successfully.
20. What are the typical use cases for Kafka in a .NET-based microservices architecture?
For beginners:
Kafka is great for scenarios where services need to communicate in real-time, such as:

Sending notifications.
Processing large amounts of data quickly (e.g., logging or metrics).
Event-driven systems where one action triggers other actions.
For experienced developers:

Event-driven architecture: Kafka enables decoupling of microservices by allowing them to communicate asynchronously through events, which is critical for scaling complex systems.
Real-time data pipelines: Kafka can be used to stream real-time data between systems, allowing for immediate processing or analytics.
Log aggregation: Kafka can collect logs from microservices and provide centralized, real-time access to logs for monitoring and troubleshooting.
Metrics and monitoring: Kafka can stream application metrics or telemetry data to monitoring systems in real-time.


------------------------------------------------------------------------------------

Step 2: Create the Producer Microservice
Create a new ASP.NET Core Web API project named KafkaProducer.

S:2.1 Install the Confluent.Kafka NuGet Package:

KafkaProducerService.cs: 

STEP 1: var config = new ProducerConfig { BootstrapServers = _bootstrapServers }; //// Set up Config OBJECT
STEP 2: var producer = new ProducerBuilder<Null, string>(config).Build() // Create a single producer instance
STEP 3: var _message =  new Message<Null, string> { Value = message }  // Initialize mesage
STEP 4: var result = await producer.ProduceAsync(_topic, _message); //Publish message with topic

using Confluent.Kafka;
using System;

public class KafkaProducerService
{

    private readonly string _bootstrapServers = "localhost:9092"; // Your Kafka Broker
    private readonly string _topic = "Tejas-topic-V1"; // Your Kafka Topic

    public async Task ProduceAsync(string message)
    {
       // Set up Config
        var config = new ProducerConfig { BootstrapServers = _bootstrapServers };

        // Create a single producer instance
        using (var producer = new ProducerBuilder<Null, string>(config).Build())
        {
            try
            {
               // Initialize mesage
                var _message =  new Message<Null, string> { Value = message }
               // send message
                var result = await producer.ProduceAsync(_topic, _message);
                Console.WriteLine($"Message sent to {result.TopicPartitionOffset}");
            }
            catch (ProduceException<Null, string> e)
            {
                Console.WriteLine($"Error producing message: {e.Message}");
            }
        }
    }
}

S:2.2 Register KafkaProducerService in Program.cs:

Open Program.cs and register the KafkaProducerService:

// Register the KafkaProducerService
builder.Services.AddSingleton<KafkaProducerService>();

S:2.3 Create the Controller:

In the Controllers folder, create a new controller KafkaController.cs:

    // POST api/kafka/send-message
    [HttpPost("send-message-teja")]
    public async Task<IActionResult> SendMessage([FromBody] string message)
    {
        await _kafkaProducerService.ProduceAsync(message);
        return Ok("Message sent to Kafka.");
    }

s:2.4 Run swagger URL: 
POST : /api/Kafka/send-message-teja
Try sending message using this api[Producing message]
-----------------------------------------
Step 3: Create the Consumer Microservice
Create a second ASP.NET Core Web API project named KafkaConsumer.

S:3.1 Install the Confluent.Kafka NuGet Package:

   public class KafkaConsumerService
   {

       public readonly string _bootstrapservers = "localhost:9092"; // Your Kafka Broker
       public readonly string _topic = "Tejas-topic-V1";  //Your Kafka topic

       public void Consume() {

           var consumeConfig = new ConsumerConfig {
               BootstrapServers = _bootstrapservers,
               GroupId = "Tejas-consumer-group",
               AutoOffsetReset = AutoOffsetReset.Earliest
           };

           using (var consumer = new ConsumerBuilder<Ignore, string>(consumeConfig).Build())
           {
               consumer.Subscribe(_topic);
               Console.WriteLine($"Consuming messages from topic: {_topic}");
               while (true)
               {
                   try
                   {
                       var consumeResult = consumer.Consume();
                       Console.WriteLine($"Message received: {consumeResult.Message.Value}");

                   }
                   catch (ConsumeException e ) {
                       Console.WriteLine($"Error consuming message: {e.Error.Reason}");

                   }

               }
           }
       }

Step 3.2 : Start the Consumer Service in Program.cs:

// Register KafkaConsumerService as a singleton
builder.Services.AddSingleton<KafkaConsumerService>();

// Start the Kafka consumer service in the background
var consumerService = app.Services.GetRequiredService<KafkaConsumerService>();
Task.Run(() => consumerService.Consume());














